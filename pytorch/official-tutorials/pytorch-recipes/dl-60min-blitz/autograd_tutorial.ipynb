{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgzA-HE2Zx6g"
      },
      "source": [
        "# AUTOGRAD: 자동 미분\r\n",
        "PyTorch의 모든 신경망의 중심에는 `autograd` 패키지가 있습니다. 먼저 이것을 가볍게 살펴본 뒤, 첫번째 신경망을 학습시켜보겠습니다.\r\n",
        "\r\n",
        "`autograd` 패키지는 Tensor의 모든 연산에 대해 자동 미분을 제공합니다. 이는 실행-기반-정의(define-by-run) 프레임워크로, 이는 코드를 어떻게 작성하여 실행하느냐에 따라 역전파가 정의된다는 뜻이며, 역전파는 학습 과정의 매 단계마다 달라집니다.\r\n",
        "\r\n",
        "더 간단한 용어로 몇 가지 예를 살펴보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dwGjo0jZ6bR"
      },
      "source": [
        "## Tensor\r\n",
        "패키지의 중심에는 `torch.Tensor` 클래스가 있습니다. 만약 `.requires_grad` 속성을 True 로 설정하면, 그 tensor에서 이뤄진 모든 연산들을 추적(track)하기 시작합니다. 계산이 완료된 후 `.backward()` 를 호출하여 모든 변화도(gradient)를 자동으로 계산할 수 있습니다. 이 Tensor의 변화도는 `.grad` 속성에 누적됩니다.\r\n",
        "\r\n",
        "- Tensor가 기록을 추적하는 것을 중단하게 하려면, `.detach()` 를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다.\r\n",
        "\r\n",
        "- 기록을 추적하는 것(과 메모리를 사용하는 것)을 방지하기 위해, 코드 블럭을 `with torch.no_grad():` 로 감쌀 수 있습니다. 이는 특히 변화도(gradient)는 필요없지만, *requires_grad=True* 가 설정되어 학습 가능한 매개변수를 갖는 모델을 평가(evaluate)할 때 유용합니다.\r\n",
        "\r\n",
        "Autograd 구현에서 매우 중요한 클래스가 하나 더 있는데, 이것은 바로 `Function` 클래스입니다.\r\n",
        "\r\n",
        "- `Tensor` 와 `Function` 은 서로 연결되어 있으며, 모든 연산 과정을 부호화(encode)하여 순환하지 않는 그래프(acyclic graph)를 생성합니다.\r\n",
        "\r\n",
        "- 각 tensor는 `.grad_fn` 속성을 갖고 있는데, 이는 `Tensor` 를 생성한 `Function` 을 참조하고 있습니다. (단, 사용자가 만든 Tensor는 예외로, 이 때 `grad_fn` 은 `None` 입니다.)\r\n",
        "\r\n",
        "도함수를 계산하기 위해서는 `Tensor` 의 `.backward()` 를 호출하면 됩니다. 만약 `Tensor` 가 스칼라(scalar)인 경우(예. 하나의 요소 값만 갖는 등)에는 `backward` 에 인자를 정해줄 필요가 없습니다. 하지만 여러 개의 요소를 갖고 있을 때는 tensor의 모양을 `gradient` 의 인자로 지정할 필요가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBlYjjdaOjc"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSkckn_VaQMx",
        "outputId": "5b891f4e-4b67-4be8-e852-9ae5351c4d74"
      },
      "source": [
        "# tensor를 생성하고 requires_grad=True 를 설정하여 연산을 기록합니다.\r\n",
        "x = torch.ones(2, 2, requires_grad=True)\r\n",
        "print(x)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1wswCWyaSNj",
        "outputId": "b473aee0-f84b-43cb-d932-2d15d64dd203"
      },
      "source": [
        "# tensor에 연산을 수행합니다\r\n",
        "y = x + 2\r\n",
        "print(y)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcK_eLICa99D",
        "outputId": "a48e1004-5e3e-447e-f326-c3eff6cbb583"
      },
      "source": [
        "# y 는 연산의 결과로 생성된 것이므로 grad_fn 을 갖습니다.\r\n",
        "print(y.grad_fn)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7f4a3c2a5c50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzGDqV--bAvL",
        "outputId": "5305a329-a8a9-4a56-d229-66636ccbe589"
      },
      "source": [
        "# y 에 다른 연산을 수행합니다.\r\n",
        "z = y * y * 3\r\n",
        "out = z.mean()\r\n",
        "\r\n",
        "print(z, out)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHtGuvazbCpc",
        "outputId": "9cd6bfd3-de6f-4e5f-aa76-c19bdf97cde4"
      },
      "source": [
        "# `.requires_grad_( ... )` 는 기존 Tensor의 `requires_grad` 값을 바꿔치기 (in-place)하여 변경합니다. 입력값이 지정되지 않으면 기본값은 `False` 입니다.\r\n",
        "\r\n",
        "a = torch.randn(2, 2)\r\n",
        "a = ((a * 3) / (a - 1))\r\n",
        "print(a.requires_grad)  # 기본값은 `False`\r\n",
        "\r\n",
        "a.requires_grad_(True)\r\n",
        "print(a.requires_grad)\r\n",
        "\r\n",
        "b = (a * a).sum()\r\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7f4a3c2aed90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4CqDtQebnvT"
      },
      "source": [
        "## 변화도(Gradient)\r\n",
        "이제 역전파(backprop)를 해보겠습니다. `out` 은 하나의 스칼라 값만 갖고 있기 때문에, `out.backward()` 는 `out.backward(torch.tensor(1.))` 과 동일합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR0WPJP2bhYO"
      },
      "source": [
        "# `out` 은 하나의 스칼라 값만 갖고 있기 때문에, `out.backward()` 는 `out.backward(torch.tensor(1.))` 과 동일\r\n",
        "out.backward()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZPQWH4bb4UM",
        "outputId": "8d156b4f-a990-4046-f8bb-831884ee762f"
      },
      "source": [
        "# 변화도 d(out)/dx를 출력합니다.\r\n",
        "print(x.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CikIa9B5b_fP"
      },
      "source": [
        "일반적으로, torch.autograd 는 벡터-야코비안 곱을 계산하는 엔진입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pboQ0XTb2oU",
        "outputId": "46017d71-fa20-4821-ff87-58bb444a1819"
      },
      "source": [
        "# 이제 벡터-야코비안 곱의 예제를 살펴보도록 한다.\r\n",
        "\r\n",
        "x = torch.randn(3, requires_grad=True)\r\n",
        "\r\n",
        "y = x * 2\r\n",
        "while y.data.norm() < 1000:\r\n",
        "    y = y * 2\r\n",
        "\r\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 891.5953,  983.8646, -147.1151], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_MiGLDwcNky"
      },
      "source": [
        "이 경우 y 는 더 이상 스칼라 값이 아닙니다. torch.autograd 는 전체 야코비안을 직접 계산할수는 없지만, 벡터-야코비안 곱은 간단히 backward 에 해당 벡터를 인자로 제공하여 얻을 수 있습니다:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxtoUygucJxw",
        "outputId": "0a07ed4b-23d4-42c2-b418-2622c291e77b"
      },
      "source": [
        "# torch.autograd 는 전체 야코비안을 직접 계산할수는 없지만,\r\n",
        "# 벡터-야코비안 곱은 간단히 backward 에 해당 벡터를 인자로 제공하여 얻을 수 있다\r\n",
        "\r\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\r\n",
        "y.backward(v)\r\n",
        "\r\n",
        "print(x.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyQYUv2xcWS-",
        "outputId": "e182827e-6f4e-48ad-bba7-d1ab326bd459"
      },
      "source": [
        "# 또한 (앞서 언급된) with torch.no_grad(): 로 코드 블럭을 감싸서\r\n",
        "# autograd가 .requires_grad=True 인 Tensor들의 연산 기록을 추적하는 것을 멈출 수 있다\r\n",
        "\r\n",
        "print(x.requires_grad)\r\n",
        "print((x ** 2).requires_grad)\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    print((x ** 2).requires_grad)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm4rfpg_cZyd",
        "outputId": "a3cdf0d3-59a7-4c30-ce74-b6f7c017c107"
      },
      "source": [
        "# 또는 .detach() 를 호출하여 내용물(content)은 같지만 require_grad가 다른 새로운 Tensor를 가져온다\r\n",
        "\r\n",
        "print(x.requires_grad)\r\n",
        "y = x.detach()\r\n",
        "print(y.requires_grad)\r\n",
        "print(x.eq(y).all())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}